{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "0681bfa1-a4a6-4bc4-88b9-46e22cfcdd36",
      "cell_type": "raw",
      "source": "Building a Retrieval-Augmented Generation (RAG) System with Weaviate\nRetrieval Augmented Generation (RAG) pipeline: relevant documents are retrieved from a vector database (like Weaviate) and combined with the user’s query to provide context for the LLM. In this hands-on workshop, we will build an end-to-end RAG system for technical Q&A using Weaviate as our vector database. We’ll walk through each step from data ingestion to generating answers with an LLM. By following along and running the code, you will gain practical experience in:\n✅ Document chunking best practices\n✅ Optimized embedding storage and retrieval\n✅ Comparing retrieval strategies (vector search, hybrid search, filtering)\n✅ Using cosine similarity as a retrieval effectiveness metric\n✅ Integrating with an LLM (GPT-4o-mini or similar) for Q&A generation\nEach section below includes explanations and code snippets. Feel free to tweak parameters and experiment – mini-exercises are provided to deepen understanding.",
      "metadata": {}
    },
    {
      "id": "ba68b3d4-7a28-4149-a2a3-18cf53222304",
      "cell_type": "markdown",
      "source": "1. Setting Up Weaviate\nFirst, we need a running Weaviate instance. You can run Weaviate locally via Docker or use a cloud-hosted instance:\nLocal (Docker): If Docker is installed, launch Weaviate with a single command. This runs Weaviate on port 8080 with default settings (no authentication, HNSW index, etc.):",
      "metadata": {}
    },
    {
      "id": "e5d702ca-6130-4c29-b8cc-11626f2c5892",
      "cell_type": "code",
      "source": "docker run -d -p 8080:8080 -p 50051:50051 semitechnologies/weaviate:latest",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d5026db2-19aa-4962-9fa1-7e9494ebd5ff",
      "cell_type": "markdown",
      "source": "(The above uses the latest Weaviate image. You can also use a specific version tag. The second port 50051 is for gRPC, not used in this workshop.)\nCloud: Alternatively, sign up for Weaviate Cloud Service (WCS) to get a free sandbox cluster. For WCS, you’ll need the cluster URL and an API key, and you should pass an API key for authentication when connecting.",
      "metadata": {}
    },
    {
      "id": "71e56071-fe9f-4077-8239-a3123a8ea05f",
      "cell_type": "markdown",
      "source": "Next, install the Python client and any other needed libraries:",
      "metadata": {}
    },
    {
      "id": "1fb8adb6-6fb9-454b-b4bf-b869be78d41a",
      "cell_type": "code",
      "source": "pip install weaviate-client openai PyMuPDF pdfplumber matplotlib numpy",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "241b7c8c-b05f-4b28-9fb0-28b8f3c640a6",
      "cell_type": "markdown",
      "source": "Now, connect to Weaviate using the Python client. For local Docker, no auth is needed. For cloud, use the URL and API key:",
      "metadata": {}
    },
    {
      "id": "ed3ccc7e-cd48-4ac6-a142-c19dd7fa4515",
      "cell_type": "code",
      "source": "import weaviate\n\n# If using local Weaviate\nclient = weaviate.Client(\"http://localhost:8080\")  \n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "478c6073-e027-4cac-a9c7-af9520dc89b0",
      "cell_type": "markdown",
      "source": "Defining the Schema\nWe’ll create a schema with a single class to store document chunks and their embeddings. In Weaviate, a class is like a collection of objects with defined properties. Our class will be called \"DocumentChunk\" and have properties for the chunk text and any metadata (like page number or chunk index). Weaviate can automatically vectorize data using modules, but since we’ll supply our own embeddings, we set the vectorizer to \"none\". We also configure the vector index (Weaviate uses an HNSW index by default) and specify the distance metric as cosine:",
      "metadata": {}
    },
    {
      "id": "82fa5bf7-6155-40e9-86eb-9aed9591ca67",
      "cell_type": "code",
      "source": "# Define class schema for document chunks\nclass_obj = {\n    \"class\": \"DocumentChunk\",\n    \"description\": \"A chunk of document text and its embedding\",\n    \"vectorizer\": \"none\",            # We'll provide our own vectors (embeddings)&#8203;:contentReference[oaicite:3]{index=3}\n    \"vectorIndexType\": \"hnsw\",       # Use HNSW index (default)\n    \"vectorIndexConfig\": {\n        \"distance\": \"cosine\"        # Cosine similarity for vector comparisons\n    },\n    \"properties\": [\n        {\n            \"name\": \"content\",\n            \"dataType\": [\"text\"],\n            \"description\": \"Text content of the document chunk\"\n        },\n        {\n            \"name\": \"page\",\n            \"dataType\": [\"int\"],\n            \"description\": \"Page number of the source PDF where this chunk is found\"\n        },\n        {\n            \"name\": \"chunkIndex\",\n            \"dataType\": [\"int\"],\n            \"description\": \"Sequential index of the chunk in the document\"\n        }\n    ]\n}\n\n# Remove class if it exists (for re-runs of the workshop notebook)\nif \"DocumentChunk\" in [c['class'] for c in client.schema.get('classes')]:\n    client.schema.delete_class(\"DocumentChunk\")\n\n# Create the class in Weaviate\nclient.schema.create_class(class_obj)\nprint(\"Schema created with classes:\", [c['class'] for c in client.schema.get('classes')])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "17fe65dd-186b-472a-ba98-519a4e8237b0",
      "cell_type": "markdown",
      "source": "This schema will allow us to store each chunk of the PDF along with its embedding vector. We included a page property to enable filtering by page, and a chunkIndex to keep track of chunk ordering. The vectorizer: none setting ensures Weaviate will not attempt to vectorize our text (we handle it externally). The distance: cosine means the HNSW index will use cosine distance for similarity search (cosine similarity is a common choice for embeddings).",
      "metadata": {}
    },
    {
      "id": "ed6bebd7-e374-4b23-bc52-10a7bae6cd28",
      "cell_type": "markdown",
      "source": "2. Loading and Chunking a Long PDF\nWith Weaviate ready, the next step is to load our large technical PDF and break it into chunks. Chunking is crucial: it balances context size (each chunk should be large enough to contain a meaningful piece of information) with relevance (chunks should be specific enough to match queries closely). We will explore multiple chunking strategies:\nFixed-length chunks (e.g. N words per chunk) – with and without overlaps.\nSentence-based splitting – chunk by whole sentences or groups of sentences.\nOverlapping vs. non-overlapping – overlapping can help preserve context between chunks.\nHybrid approaches – e.g. split by paragraph or section, ensuring a minimum length.\nLet’s start by reading the PDF. We’ll use PyMuPDF (imported as fitz) to extract text from each page. We’ll also demonstrate chunking by page to capture page-wise metadata:",
      "metadata": {}
    },
    {
      "id": "a8514a9a-8910-43ae-87f1-0f6e5821dde6",
      "cell_type": "code",
      "source": "import fitz  # PyMuPDF\nimport math\n\npdf_path = \"technical_spec.pdf\"  # replace with your PDF file path\ndoc = fitz.open(pdf_path)\n\nall_chunks = []\nchunk_size = 100   # example size: 100 words per chunk\noverlap = 20       # example overlap: 20 words\n\nchunk_index = 0\nfor page_num in range(len(doc)):\n    page = doc.load_page(page_num)\n    text = page.get_text().strip()\n    if not text:\n        continue  # skip blank pages\n    # Split the page text into chunks of ~100 words with 20-word overlap\n    words = text.split()\n    for i in range(0, len(words), chunk_size - overlap if overlap else chunk_size):\n        chunk_words = words[i : i + chunk_size]\n        chunk_text = \" \".join(chunk_words)\n        # Store the chunk with metadata\n        chunk_obj = {\n            \"content\": chunk_text,\n            \"page\": page_num,\n            \"chunkIndex\": chunk_index\n        }\n        all_chunks.append(chunk_obj)\n        chunk_index += 1\n\nprint(f\"Total chunks created: {len(all_chunks)}\")\nprint(\"Sample chunk:\\n\", all_chunks[0]['content'][:250], \"...\")\nprint(\"Metadata of sample chunk:\", {k: all_chunks[0][k] for k in ['page','chunkIndex']})",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "89b49eb8-1920-4bc3-8acb-2fe20e950926",
      "cell_type": "markdown",
      "source": "In the above code, we iterate through each page, extract text, then create chunks of ~100 words, with a 20-word overlap between consecutive chunks. Overlapping chunks by some fraction (here 20%) helps preserve context that spans chunk boundaries​\nWEAVIATE.IO\n. We stored each chunk in a dictionary with the text and metadata (page number and a sequential index). Chunking strategies: The fixed-size approach above is one strategy. Depending on your document and use case, you might try different methods:\nFixed-length (non-overlapping): set overlap = 0 in the code above to make disjoint chunks of chunk_size words.\nSentence-based splitting: maintain whole sentences in each chunk. For example, we can split the text by sentence and then group sentences until a length threshold is reached:",
      "metadata": {}
    },
    {
      "id": "a03b0cc9-12e9-4584-94b9-727885e9123e",
      "cell_type": "code",
      "source": "import re\ndef chunk_by_sentence(text, max_words=100):\n    # Split text into sentences (naively by period/question/exclamation)\n    sentences = re.split(r'(?<=[.?!])\\s+', text)\n    chunks = []\n    current_chunk = []\n    current_count = 0\n    for sent in sentences:\n        word_count = len(sent.split())\n        if current_count + word_count > max_words:\n            # finalize the current chunk and start a new one\n            chunks.append(\" \".join(current_chunk).strip())\n            current_chunk = []\n            current_count = 0\n        current_chunk.append(sent)\n        current_count += word_count\n    # add the last chunk\n    if current_chunk:\n        chunks.append(\" \".join(current_chunk).strip())\n    return chunks\n\n# Example: chunk first page text by sentence grouping\npage0_text = doc.load_page(0).get_text()\nsentence_chunks = chunk_by_sentence(page0_text, max_words=100)\nprint(f\"Page 0 split into {len(sentence_chunks)} chunks by sentences.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3c305c63-8615-4714-9477-d11ee7787483",
      "cell_type": "markdown",
      "source": "This function ensures chunks end at sentence boundaries and contain up to ~100 words. Sentence-based chunks might be more semantically coherent, which can help the embeddings capture meaning better.",
      "metadata": {}
    },
    {
      "id": "1024a5b4-ebb5-4ae8-bcfa-48f0c1c05715",
      "cell_type": "markdown",
      "source": "Paragraph-based or Hybrid: Many technical PDFs have clear paragraph or section breaks. We can split on double newlines (\\n\\n) to get paragraphs, then merge smaller paragraphs so each chunk has at least a minimum number of words. For example:",
      "metadata": {}
    },
    {
      "id": "aa0c0bad-54fa-44dd-9067-26464f7b00de",
      "cell_type": "code",
      "source": "def chunk_by_paragraph(text, min_words=50):\n    paras = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n    chunks = []\n    buffer = \"\"\n    for para in paras:\n        if len(para.split()) < min_words:\n            # accumulate small paragraphs in buffer\n            buffer += \" \" + para\n        else:\n            # if buffer has content, append it before the current para\n            if buffer:\n                combined = buffer + \" \" + para\n                chunks.append(combined.strip())\n                buffer = \"\"\n            else:\n                chunks.append(para)\n    if buffer:\n        chunks.append(buffer.strip())\n    return chunks\n\n# Example: chunk first page by paragraph\npara_chunks = chunk_by_paragraph(page0_text, min_words=50)\nprint(f\"Page 0 split into {len(para_chunks)} chunks by paragraph (with min_words=50).\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d99e6d55-5228-4925-944a-905499acfb35",
      "cell_type": "markdown",
      "source": "This approach tries to keep paragraphs intact and ensures no chunk is too short (small paragraphs get merged with neighbors). It’s a simple heuristic for a hybrid strategy.",
      "metadata": {}
    },
    {
      "id": "bedb0903-9e24-4fe4-b4bc-762bb39b852c",
      "cell_type": "markdown",
      "source": "Handling tables and diagrams (optional): Technical PDFs may contain tables or diagrams that are not plain text. By default, our text extraction might skip or mishandle these. For a thorough solution, you could incorporate OCR or specialized parsing:\nUse the pdfplumber library to detect tables (via page.extract_table) and include them as text (e.g., CSV format in the chunk).\nUse an OCR or the unstructured library to handle images/diagrams and get a text description. In this workshop, we focus on text chunks. You can consider these enhancements as optional exercises (the code is structured so you can plug in additional parsing if needed).",
      "metadata": {}
    },
    {
      "id": "dd49100d-901f-489b-9ae6-3a61d1000707",
      "cell_type": "markdown",
      "source": "Mini-exercise: Try adjusting the chunk_size and overlap parameters, or switch to the sentence-based function, and observe how the number and content of chunks change. For instance, compare chunk_size=50 vs 200, or overlap=0 vs 20%. Smaller, non-overlapping chunks result in more segments with narrower focus, whereas larger or overlapping chunks carry more context. An optimal chunking strikes a balance – one forum discussion notes that larger chunks provide more context but can dilute specificity of the embedding. Experiment and see what works best for retrieval accuracy.",
      "metadata": {}
    },
    {
      "id": "3321f7fd-d446-4602-bc8e-5f692c5d2f64",
      "cell_type": "markdown",
      "source": "3. Generating and Storing Embeddings in Weaviate\nNow that we have chunks of the document, the next step is to convert each chunk into a vector embedding and store them in Weaviate. We will use OpenAI’s text embedding model (for example, text-embedding-ada-002, which produces 1536-dimensional embeddings) to generate vectors for each chunk. You can replace this with other providers or models (we’ll mention alternatives in comments). Before running embedding, make sure you have an API key for OpenAI (or your chosen provider). Set up the OpenAI API:",
      "metadata": {}
    },
    {
      "id": "2091c1fb-cc5f-4dca-8fc3-74d3667424d0",
      "cell_type": "code",
      "source": "import openai\n\nopenai.api_key = \"YOUR_OPENAI_API_KEY\"  # **Replace with your key** or load from environment\nmodel_name = \"text-embedding-ada-002\"   # OpenAI embedding model (Ada v2)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ed30eed2-a5b1-47d8-900f-ccb78a9e2e9a",
      "cell_type": "markdown",
      "source": "Now, iterate through the chunks, compute the embedding for each, and send the data into Weaviate. We’ll use Weaviate’s batch API for efficiency. We’ll also allow tweaking preprocessing (for example, lowercasing the text or removing special characters) if needed before embedding – though often it’s not necessary for modern models:",
      "metadata": {}
    },
    {
      "id": "51529dd8-d2ab-47e0-ac05-b7d28750f056",
      "cell_type": "code",
      "source": "# (Optional) Preprocessing function, e.g., to normalize text (currently just a placeholder)\ndef preprocess_text(text):\n    return text.strip()  # We could add lowercasing, remove punctuation, etc., if needed.\n\n# Prepare batch import\nbatch_size = 100\nclient.batch.configure(batch_size=batch_size, dynamic=True)  # dynamic batching adapts to payload size\n\nfor i, chunk_obj in enumerate(all_chunks):\n    text = preprocess_text(chunk_obj[\"content\"])\n    # Generate embedding vector using OpenAI\n    try:\n        emb_response = openai.Embedding.create(input=text, model=model_name)\n    except Exception as e:\n        raise RuntimeError(f\"Embedding API call failed at chunk {i}: {e}\")\n    vector = emb_response['data'][0]['embedding']\n    # Add to Weaviate batch\n    client.batch.add_data_object(\n        data_object={\n            \"content\": text,\n            \"page\": chunk_obj[\"page\"],\n            \"chunkIndex\": chunk_obj[\"chunkIndex\"]\n        },\n        class_name=\"DocumentChunk\",\n        vector=vector\n    )\n    # Send batch every 100 objects (to avoid too large batches in memory)\n    if (i + 1) % batch_size == 0:\n        client.batch.create_objects()\n        print(f\"{i+1} chunks indexed...\")\n# Flush remaining\nclient.batch.create_objects()\nprint(f\"Finished indexing {len(all_chunks)} chunks.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d500bb0c-38ab-46be-a14d-fbb3e1bb75e4",
      "cell_type": "markdown",
      "source": "This code will take each chunk, get a 1536-dim embedding from OpenAI, and batch import it into Weaviate as a DocumentChunk object with the vector attached. We configured vectorizer: none earlier, so Weaviate knows these vectors are the ones to index (it won’t try to generate its own).",
      "metadata": {}
    },
    {
      "id": "a7ae9d6d-6d5f-4481-9df1-eaf182c38d6c",
      "cell_type": "markdown",
      "source": "Embedding alternatives: The above uses OpenAI. You can switch to other providers:\nCohere: e.g. use cohere.Client to embed text (co.embed(texts=[text]) returns vectors). Make sure to adjust the vector dimension and schema if using a model with different output size.\nHugging Face Transformers: use a SentenceTransformer model or a pipeline. For example:",
      "metadata": {}
    },
    {
      "id": "7fa21c09-8bae-4698-a0d8-0c11f362df92",
      "cell_type": "code",
      "source": "# Using a SentenceTransformer model from Hugging Face\nfrom sentence_transformers import SentenceTransformer\nhf_model = SentenceTransformer('all-MiniLM-L6-v2')  # 384-dim embeddings\nvector = hf_model.encode(text).tolist()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "634113cf-0670-4e01-9167-7266387da9f9",
      "cell_type": "markdown",
      "source": "If you use a 384-dim model like MiniLM, update vectorIndexConfig in the schema or ensure all vectors are the same length (Weaviate requires consistent dimensions). Also consider the quality: smaller models may be faster but might reduce accuracy for complex technical text.\nLocal GPU models: if you have GPU and a larger model (e.g., OpenAI’s text-embedding-ada-002 clone or similar), you can use that. The code structure remains the same; only the embedding function changes.",
      "metadata": {}
    },
    {
      "id": "b2c9894d-7301-4709-9e40-1f16a3ec1144",
      "cell_type": "markdown",
      "source": "Tweakable settings: Feel free to modify model_name, adjust batch_size for indexing, or add preprocessing (e.g., removing stopwords) before embedding. These can impact performance:\nUsing a different model may require re-running the schema setup if dimensions differ.\nPreprocessing might slightly improve embeddings if your text contains a lot of irrelevant characters (but often the embedding model is robust enough without heavy cleaning).",
      "metadata": {}
    },
    {
      "id": "d2c40cba-05e4-4509-ac60-968e8eee87ce",
      "cell_type": "markdown",
      "source": "After running the above, your Weaviate instance should now be populated with one object per chunk, each with an embedding. We can verify by checking object count or retrieving a sample object:",
      "metadata": {}
    },
    {
      "id": "2fe0b7e8-ea0e-42eb-afa4-30001488d586",
      "cell_type": "code",
      "source": "# Verify data indexed\nobj_count = client.query.aggregate(\"DocumentChunk\").with_meta_count().do()\nprint(\"Object count in Weaviate:\", obj_count[\"data\"][\"Aggregate\"][\"DocumentChunk\"][0][\"meta\"][\"count\"])\n\n# Retrieve a sample object to verify content\nresult = client.query.get(\"DocumentChunk\", [\"content\", \"page\", \"_additional {vector}\"]).with_limit(1).do()\nprint(\"Sample stored object:\", result[\"data\"][\"Get\"][\"DocumentChunk\"][0])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "219da1d2-fa59-4262-aef3-6cf2ae57cdce",
      "cell_type": "markdown",
      "source": "With our vector database populated, we can perform semantic search to retrieve relevant chunks for a given query. Weaviate supports multiple query types:\nVector similarity search – find chunks with embeddings nearest to the query embedding (semantic search).\nKeyword (BM25) search – find chunks by lexical match of query terms (like traditional search).\nHybrid search – combines vector similarity and keyword relevance.\nFiltering – restrict results by metadata (e.g., only certain pages or sections).\nThese capabilities can be combined. As the Weaviate docs note, you can use similarity, keyword, and hybrid searches, along with filtering, to find the information you need​\nWEAVIATE.IO\n. We’ll explore pure vector vs hybrid, and demonstrate a metadata filter. First, define a sample user query. For example, our use-case query (from the prompt) is:",
      "metadata": {}
    },
    {
      "id": "47e6961c-080b-4f64-bfac-e7102a43203c",
      "cell_type": "code",
      "source": "query_text = (\"Generate an AT command sequence that will attach the device to an \"\n              \"LTE network using eDRX with 81 seconds cycle interval, periodically send 100 bytes of data using HTTPs, \"\n              \"and immediately release the connection using RAI.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e2798adc-c82d-4965-87ed-16ecb85999a9",
      "cell_type": "markdown",
      "source": "This is a complex request that likely spans multiple parts of the technical specification (network attachment, eDRX settings, HTTP data sending, and RAI usage). Our goal is to retrieve the most relevant chunks from the document that contain information about these topics. ",
      "metadata": {}
    },
    {
      "id": "2032f17e-2150-431c-85cb-ff7d7bfcf51a",
      "cell_type": "markdown",
      "source": "Next, get the query’s embedding vector (using the same model as we did for the documents, to ensure vector space alignment):",
      "metadata": {}
    },
    {
      "id": "15d3943e-9b42-4504-aac0-7cdfb15fa7bf",
      "cell_type": "code",
      "source": "# Embed the query text to a vector\nquery_emb = openai.Embedding.create(input=query_text, model=model_name)\nquery_vector = query_emb['data'][0]['embedding']",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c92cc9ca-9a4d-4262-a4f4-d70a263494aa",
      "cell_type": "markdown",
      "source": "Now we can query Weaviate. We’ll try three approaches: \na. Pure Vector Search: This uses only the embedding similarity (cosine similarity) to rank results. Weaviate’s GraphQL (via the Python client) provides a nearVector operator where we pass the query vector. We’ll request the content, page, chunkIndex, and the similarity score. Since we set distance: cosine in the schema, Weaviate will return a distance (0 = identical, higher = less similar). We can ask for distance or certainty (Weaviate uses certainty in some contexts, which is 1 - distance for cosine).",
      "metadata": {}
    },
    {
      "id": "c24b0d41-b096-432e-8be8-edd3514eca3e",
      "cell_type": "code",
      "source": "# Pure vector similarity search (cosine)\nresults_vector = client.query.get(\"DocumentChunk\", \n                                  [\"content\", \"page\", \"chunkIndex\", \"_additional {distance}\"]\n                                 ).with_near_vector({\"vector\": query_vector}).with_limit(5).do()\n\nchunks_vector = results_vector[\"data\"][\"Get\"][\"DocumentChunk\"]\nfor i, res in enumerate(chunks_vector, 1):\n    text_snippet = res[\"content\"][:100].replace(\"\\n\", \" \")\n    print(f\"{i}. [Page {res['page']}] {text_snippet}... (distance={res['_additional']['distance']:.3f})\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "770e8963-b287-4fc7-8b24-029d06ca826f",
      "cell_type": "markdown",
      "source": "This will output the top 5 most similar chunks by vector embedding. Lower distance means higher similarity. You should see chunks that mention terms like “LTE attach”, “eDRX 81 seconds”, “100 bytes HTTP” or “RAI” if those are in the document. If the distance values are all relatively low (say 0.2–0.4), it indicates a strong semantic match for those concepts.",
      "metadata": {}
    },
    {
      "id": "07f72fab-fea7-4e6f-a32a-a8932a880501",
      "cell_type": "markdown",
      "source": "b. Hybrid Search: Hybrid search combines keyword matching with vector similarity for a more robust retrieval. For example, if the query contains specific terms (like “RAI” or “HTTPs”) that appear in the document, BM25 keyword search can ensure those chunks are considered, even if their overall embedding similarity might be slightly lower. Weaviate’s hybrid operator allows a mix of vector and text search. We can provide the query text and the query vector together. An alpha parameter (0 to 1) controls the balance (alpha=0 is purely keyword, alpha=1 is purely vector; 0.5 gives equal weight):",
      "metadata": {}
    },
    {
      "id": "e8b72d60-4b3f-4c9f-9012-56f4c98f900a",
      "cell_type": "code",
      "source": "# Hybrid search: combine vector similarity and keyword (BM25) search\nresults_hybrid = client.query.get(\"DocumentChunk\", \n                                  [\"content\", \"page\", \"chunkIndex\", \"_additional {score}\"]\n                                 ).with_hybrid(query=query_text, vector=query_vector, alpha=0.5).with_limit(5).do()\n\nchunks_hybrid = results_hybrid[\"data\"][\"Get\"][\"DocumentChunk\"]\nfor i, res in enumerate(chunks_hybrid, 1):\n    snippet = res[\"content\"][:100].replace(\"\\n\", \" \")\n    print(f\"{i}. [Page {res['page']}] {snippet}... (hybrid_score={res['_additional']['score']:.3f})\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "19f4dbf6-74df-4b6b-8578-4d182e22df04",
      "cell_type": "markdown",
      "source": "In the hybrid result, _additional {score} is a fusion score (higher = more relevant) that combines vector and keyword relevance. You might observe that hybrid results include chunks that contain the exact query terms (e.g., a chunk explicitly mentioning “RAI” or “eDRX”) even if those weren’t the top pure vector hits. Hybrid search is powerful when you want to ensure certain keywords are present in results while still leveraging semantic similarity.",
      "metadata": {}
    },
    {
      "id": "87766e96-ddd1-474e-a3d4-5f7cc8004d63",
      "cell_type": "markdown",
      "source": "c. Metadata Filtering: Sometimes you want to restrict search to a subset of the data. For instance, if our PDF had multiple sections or if we only trust certain pages, we can apply a filter. Weaviate allows filtering on properties using .with_where in the query. We stored page numbers, so as an example, we can limit the search to the first 50 pages:",
      "metadata": {}
    },
    {
      "id": "9f7b8405-c34e-4595-9d81-80107a0a4539",
      "cell_type": "code",
      "source": "# Example filter: only consider chunks from pages 0-49\npage_filter = {\n    \"path\": [\"page\"],\n    \"operator\": \"LessThan\",\n    \"valueInt\": 50\n}\nresults_filtered = client.query.get(\"DocumentChunk\", [\"content\", \"page\", \"_additional {distance}\"])\\\n                    .with_near_vector({\"vector\": query_vector})\\\n                    .with_where(page_filter)\\\n                    .with_limit(5).do()\n\nchunks_filtered = results_filtered[\"data\"][\"Get\"][\"DocumentChunk\"]\nprint(f\"Found {len(chunks_filtered)} results with page filter:\")\nfor res in chunks_filtered:\n    print(f\"- Page {res['page']} (distance={res['_additional']['distance']:.3f}): {res['content'][:80]}...\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "06012f95-2f2d-495e-844c-dac804451792",
      "cell_type": "markdown",
      "source": "This query will only return chunks from pages 0–49 that are closest to the query. If the relevant info was in later pages, those would be excluded, demonstrating the effect of the filter. You can filter on any property – for example, if we had a section or chapter property, we could filter by that. Filtering is useful to narrow scope (e.g., “only search in the AT commands reference section”). Comparison of methods: In practice, you might try pure vector vs hybrid to see which gives better results for your queries. Pure vector search may surface semantically relevant chunks that don’t share exact wording with the query, while hybrid can improve precision when specific terms are important. Weaviate’s hybrid search essentially fuses BM25 and vector results, which often yields the best of both worlds.",
      "metadata": {}
    },
    {
      "id": "7f3fad03-dd6c-4dd3-b0f4-d8ad2fe66d19",
      "cell_type": "markdown",
      "source": "Mini-exercise: Try running both the pure vector and hybrid searches for the sample query. Do you notice any differences in the returned chunks or their order? Which method retrieves the chunk about “RAI” first? Now adjust the alpha in hybrid (e.g., 0.3 vs 0.7) to put more weight on keywords or vectors and see how results change. Also experiment with the metadata filter – for example, change the filter to a different page range or remove it entirely to see the unfiltered results.",
      "metadata": {}
    },
    {
      "id": "4124762b-880b-4d40-ba9e-c087cdc59d98",
      "cell_type": "markdown",
      "source": "5. Evaluating Retrieval Performance\nTo measure how well our retrieval is doing, we can calculate similarity scores between the query and the retrieved chunks. Since we’re using cosine similarity, a higher cosine similarity (closer to 1.0) means a more relevant chunk (in terms of embedding). Weaviate already gives us a distance or score, but we can also recompute cosine similarity manually to double-check or to aggregate results.\nLet’s take the results from the pure vector search in section 4a and compute cosine similarities for each retrieved chunk",
      "metadata": {}
    },
    {
      "id": "c7ad6145-0d20-4f43-bed3-8b69d452a1c7",
      "cell_type": "code",
      "source": "import numpy as np\n\ndef cosine_sim(vec1, vec2):\n    vec1, vec2 = np.array(vec1), np.array(vec2)\n    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n\n# Get the top vector search results again (including their chunkIndex for lookup)\nresults = client.query.get(\"DocumentChunk\", [\"content\", \"chunkIndex\", \"_additional {distance}\"])\\\n            .with_near_vector({\"vector\": query_vector}).with_limit(5).do()\nchunks = results[\"data\"][\"Get\"][\"DocumentChunk\"]\n\n# Assuming we still have the list of chunk vectors from indexing step, let's build a lookup by chunkIndex:\n# (If not stored, we could also ask Weaviate for vectors via _additional {vector} in the query above)\n# For demonstration, let's retrieve vectors for these chunks in a second query:\nidx_list = [res[\"chunkIndex\"] for res in chunks]\nvector_results = client.query.get(\"DocumentChunk\", [\"chunkIndex\", \"_additional {vector}\"])\\\n                  .with_where({\"path\": [\"chunkIndex\"], \"operator\": \"ContainedIn\", \"valueInt\": idx_list})\\\n                  .with_limit(len(idx_list)).do()\nvector_data = vector_results[\"data\"][\"Get\"][\"DocumentChunk\"]\n# Build a map from chunkIndex to vector\nchunk_vectors = {obj[\"chunkIndex\"]: obj[\"_additional\"][\"vector\"] for obj in vector_data}\n\n# Calculate cosine similarity between query and each retrieved chunk vector\nsims = []\nfor res in chunks:\n    idx = res[\"chunkIndex\"]\n    vec = chunk_vectors.get(idx)\n    if vec:\n        sim = cosine_sim(query_vector, vec)\n        sims.append(sim)\n        snippet = res[\"content\"][:60].replace(\"\\n\", \" \")\n        print(f\"Chunk {idx} (cosine sim={sim:.3f}): {snippet}...\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c9ddaf4e-b3f0-494a-9d98-38a4ebe856db",
      "cell_type": "markdown",
      "source": "This will print the cosine similarity for each of the top 5 chunks. You can compare these similarities to the distances or scores returned by Weaviate:\nIf using cosine distance: cosine_similarity = 1 - distance (approximately, since cosine distance = 1 - cos sim for normalized vectors).\nIf using certainty: certainty roughly correlates directly with similarity (higher certainty = more similar).",
      "metadata": {}
    },
    {
      "id": "157359c2-46bd-4d14-98a3-68fc3d32fe8a",
      "cell_type": "markdown",
      "source": "We can also compute an average cosine similarity of the top results as a simple relevance metric. For example:",
      "metadata": {}
    },
    {
      "id": "4dd3843c-e2c7-43a5-9663-2044ac941892",
      "cell_type": "code",
      "source": "if sims:\n    avg_sim = sum(sims) / len(sims)\n    print(f\"Average cosine similarity of top {len(sims)} results: {avg_sim:.3f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e4969c14-a223-44b5-a92f-67877dac2140",
      "cell_type": "markdown",
      "source": "This average can indicate how tightly clustered the top results are around the query in vector space. A higher average might mean the query hits a very specific concept (all top results are similar to the query and to each other), whereas a lower average might mean the results are more loosely related or the query is broad. Dynamic re-ranking: With this setup, you can tweak chunking or embedding and quickly recompute the similarity metrics to see the effect. For instance, if you re-chunk the document with a different strategy and re-index, you can run the same query and measure the new average cosine similarity or check if the relevant chunk now ranks higher.",
      "metadata": {}
    },
    {
      "id": "e322b1d1-04e7-4312-9c31-1a8a72e856b1",
      "cell_type": "markdown",
      "source": "Mini-exercise: After modifying your chunking approach or embedding model, run the retrieval and similarity computation again. Does the average top-k similarity increase or decrease? Examine the individual similarities – ideally, the truly relevant chunks should show the highest similarity to the query. If your changes improve the retrieval, you might see the target information chunk moving up to rank 1 with a higher score. This iterative approach helps in tuning the system.",
      "metadata": {}
    },
    {
      "id": "33f2ba07-af0e-4f35-9340-297fd800e13c",
      "cell_type": "markdown",
      "source": "6. Integrating with GPT-4o-mini for RAG-Based Answer Generation\nRetrieval alone isn’t enough – we want to use the retrieved chunks to answer the user’s question. In a RAG pipeline, the relevant chunks are fed into a generative model (like GPT-4 or a smaller variant) as context. The model then crafts an answer that draws from that context. For our example query about the AT command sequence, let’s assume we have retrieved several chunks covering:\nHow to attach to LTE with eDRX and the cycle interval,\nHow to send data over HTTP,\nHow to release connection using RAI.",
      "metadata": {}
    },
    {
      "id": "f8361157-d832-4839-bb1f-a3e62b7c89fd",
      "cell_type": "markdown",
      "source": "We will now prompt an LLM with these chunks. We’ll use OpenAI’s GPT API for demonstration (you can use GPT-4 if you have access, or GPT-3.5 as a proxy). We refer to “GPT-4o-mini” as our hypothetical model – in practice, this could be an approximation of GPT-4 or any suitable LLM that can handle the prompt length. First, collect the top chunks’ content as context:",
      "metadata": {}
    },
    {
      "id": "16871bf1-cb57-4848-905d-6f2db8d9d723",
      "cell_type": "code",
      "source": "# Let's use the hybrid search results (assuming it provided well-rounded context) from earlier\ncontext_chunks = [res[\"content\"] for res in chunks_hybrid]  # top 5 from hybrid search\n# Or use chunks_vector if you prefer pure vector results\n\n# Prepare a single string with all context\ncontext_text = \"\\n\\n\".join(context_chunks)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "51d7c989-834d-4aa9-9e9e-73c633584dda",
      "cell_type": "markdown",
      "source": "Now, construct the prompt for the LLM. A common strategy is to include the context and then the query, instructing the model to use the context to answer. For example:",
      "metadata": {}
    },
    {
      "id": "94814e9d-b9fa-4df6-8b43-f6e15b892cdd",
      "cell_type": "code",
      "source": "prompt = (f\"You are a helpful AI assistant. You are given the following technical documentation context:\\n\\n\"\n          f\"{context_text}\\n\\n\"\n          f\"Using this information, please answer the question:\\n\"\n          f\"{query_text}\\n\\n\"\n          f\"Provide a step-by-step AT command sequence with explanations.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "863560c2-4fea-441d-9980-9421c3554a27",
      "cell_type": "markdown",
      "source": "In the prompt above, we set a role (assistant with context) and explicitly ask for a step-by-step AT command sequence, since the query expects a sequence of commands. We included the retrieved chunks in the prompt. It’s important to note that if the context is very large, you must ensure it fits within the model’s token limit (for GPT-3.5/4 this is usually a few thousand tokens, and our 5 chunks likely fit). Now we send this prompt to the model:",
      "metadata": {}
    },
    {
      "id": "12c7ef95-41ef-49cf-bd42-466b3a0cd9de",
      "cell_type": "code",
      "source": "# Use OpenAI ChatCompletion (GPT)\nresponse = openai.ChatCompletion.create(\n    model=\"gpt4o-mini\", \n    messages=[{\"role\": \"user\", \"content\": prompt}]\n)\nanswer = response[\"choices\"][0][\"message\"][\"content\"]\nprint(\"AI Answer:\\n\", answer)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "90d09299-7c42-4028-bf6e-2f260ca87926",
      "cell_type": "markdown",
      "source": "This will output the generated answer. Ideally, the answer will list a series of AT commands (like maybe AT+CGATT=1 to attach, AT+CEDRXS=... to set eDRX, commands to send data over HTTP, and AT+RAI=... or an equivalent command to release with RAI) along with some explanation.",
      "metadata": {}
    },
    {
      "id": "b8f77eb5-8ddc-4e80-9271-6de83230ce1a",
      "cell_type": "markdown",
      "source": "Prompt refinement: The initial answer might not be perfectly formatted or may miss some detail. You can refine the prompt by:\nAdding instructions such as “if the context does not contain the info, say you don’t know” to avoid hallucination.\nChanging the format request (e.g., “provide just the AT commands without extra text” if you want a concise output).\nAdding a system message in the ChatCompletion for clearer role (OpenAI’s API allows a system message like: {\"role\": \"system\", \"content\": \"You are an expert IoT device assistant...\"}).\nFor instance, to ensure the answer is an actual command sequence, you might do:",
      "metadata": {}
    },
    {
      "id": "e33ddee6-4669-46b8-a58c-5401deadae8d",
      "cell_type": "code",
      "source": "messages = [\n    {\"role\": \"system\", \"content\": \"You are an expert IoT modem assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\nresponse = openai.ChatCompletion.create(model=\"gpt-4\", messages=messages)\nprint(response[\"choices\"][0][\"message\"][\"content\"])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f35cf799-1f39-4b84-bf51-da84b8114da3",
      "cell_type": "markdown",
      "source": "Alternative LLMs: If you don’t have access to OpenAI’s API or prefer local models, you can use Hugging Face Transformers. For example, a suitable instruct-tuned model (like a smaller GPT-J or FLAN-T5) can be invoked with the transformers pipeline:",
      "metadata": {}
    },
    {
      "id": "35c8e708-089f-4de9-b04b-02674ae86065",
      "cell_type": "code",
      "source": "# Using a local model via HuggingFace (ensure the model fits in memory and is suitable for Q&A)\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nmodel_name = \"tiiuae/falcon-7b-instruct\"  # example open model (7B parameters)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ngenerator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nresponse = generator(prompt, max_length=500, do_sample=False)\nprint(response[0]['generated_text'])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3d83fb27-6ea1-4046-b5c7-6307e0d033e5",
      "cell_type": "markdown",
      "source": "Be aware that smaller models may not produce as accurate or detailed answers as GPT-3.5/4, especially for technical queries. However, they can be used for experimentation if API access is an issue.",
      "metadata": {}
    },
    {
      "id": "97e06563-faf7-4d07-967d-6844d0230d1d",
      "cell_type": "markdown",
      "source": "Mini-exercise: Run the end-to-end query for the provided sample question and examine the output. Does the answer use information from the retrieved chunks correctly? If something is off, try modifying the prompt (e.g., ask for clarification or a different format). You could also test another query: for example, “How do I enable Power Saving Mode (PSM) on this device via AT commands?” (assuming the document covers PSM). See if the pipeline retrieves relevant context and if the LLM can answer it. This will validate your RAG system on a different question.",
      "metadata": {}
    },
    {
      "id": "246c17a4-40a7-4dcc-a55f-a254c9dac8ef",
      "cell_type": "markdown",
      "source": "7. Experimentation and Mini-Exercises\nBy now, we have a working RAG pipeline. This is a great point to pause and experiment. Here are some mini-exercises and ideas to try, to deepen your understanding and possibly improve the system:\n\nChunking strategies: Change the chunking approach and re-index the data. For example, use the sentence-based chunking or a larger chunk_size. How does that affect retrieval? Does the answer quality improve or degrade? Perhaps overlapping chunks gave redundant results – try without overlap and see if you get a broader range of information in the top hits.\n\nEmbedding model choices: If you have access to other embedding models (Cohere, HuggingFace, etc.), try using them. You may need to adjust the Weaviate schema if the vector dimension changes. After indexing with a different model, run the same query and compare the results. Are the retrieved chunks more or less relevant? This can show how embedding quality impacts downstream QA.\n\nVector vs Keyword vs Hybrid: We demonstrated vector and hybrid search. You can also simulate a pure keyword search by using .with_hybrid(query=query_text, alpha=0) which would rank purely by BM25 text relevance. Try this and see what results you get (likely, chunks containing literal “LTE”, “eDRX”, “RAI” will surface). Compare the answers the LLM gives when using pure keyword context vs pure vector context. Which answer is more accurate?\n\nMetadata filtering: If your document had labeled sections (say “Section 5: Network Attach”), you could add that as metadata and then filter queries to only search within a specific section when you know where the answer should come from. Try adding a dummy filter (like page range) as we did, and also try no filter – observe if irrelevant chunks from elsewhere ever creep in. This teaches when filtering can enhance precision.\n\nMultiple queries testing: Write a small list of queries related to the document’s content (for example, other AT command scenarios if it’s a modem spec). Automate retrieving answers for each. Evaluate the results – this can highlight strengths and weaknesses of your RAG system. You might find certain queries where the retrieval needs tuning or the LLM needs a better prompt.\n\nPerformance and scaling: If your PDF is hundreds of pages, you might have thousands of chunks. The code as written should handle it, but monitor performance. Weaviate’s vector search is very fast even for large corpora, but embedding generation (if using an external API) could be a bottleneck. You could experiment with batch embedding (OpenAI allows up to 2048 tokens per request and you can send multiple texts in one API call to speed it up). Also, adjusting the batch_size in Weaviate import can optimize throughput.\n\nBy trying these experiments, you’ll get a feel for how each component affects the whole system. This kind of iterative experimentation is typical in building real-world RAG applications.",
      "metadata": {}
    },
    {
      "id": "8740fba4-a205-493e-aea6-47ef9fa33c1f",
      "cell_type": "markdown",
      "source": "8. Visualization of Retrieval Effectiveness\nTo better understand our retrieval results, it’s helpful to visualize the similarity scores. We will plot the cosine similarities of the top retrieved chunks to see the distribution and drop-off. For example, let’s take the similarities we computed in section 5 for the top 5 vector search results and visualize them:",
      "metadata": {}
    },
    {
      "id": "a8fd5aa0-3e55-4db0-b7fd-4a5bb062adfc",
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\n\n# Assuming sims list from earlier (cosine similarities of top results) is available\nsims_sorted = sorted(sims, reverse=True)\nplt.figure(figsize=(6,4))\nplt.bar(range(1, len(sims_sorted)+1), sims_sorted, color='skyblue')\nplt.xlabel('Result Rank')\nplt.ylabel('Cosine Similarity to Query')\nplt.title('Similarity of Top Retrieved Chunks')\nplt.xticks(range(1, len(sims_sorted)+1))\nplt.ylim(0, 1.0)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f94d42f5-1660-4e0f-b7a7-7bd3d9d660b4",
      "cell_type": "markdown",
      "source": "This bar chart shows the cosine similarity for the 1st, 2nd, 3rd, etc., ranked chunks. You might see something like: the top result has similarity ~0.95, second ~0.90, and then a drop to ~0.8 for the later ones (this is just an illustration; actual values depend on your data and query). A sharp drop after the first result could mean the query has one extremely relevant chunk and the rest are less so. A more gradual decline suggests the query spans multiple chunks or the information is spread out.",
      "metadata": {}
    },
    {
      "id": "5fbf820e-e03a-4172-a323-1184a75d817c",
      "cell_type": "markdown",
      "source": "If you want a bigger picture, you can retrieve, say, the top 10 or 20 results and plot their similarities or distances. A histogram of distances could also be insightful:",
      "metadata": {}
    },
    {
      "id": "8a36f57b-c8f8-493e-b91a-e544843a71ea",
      "cell_type": "code",
      "source": "# Get top 20 results distances\nresults20 = client.query.get(\"DocumentChunk\", [\"_additional {distance}\"]).with_near_vector({\"vector\": query_vector}).with_limit(20).do()\ndistances = [obj[\"_additional\"][\"distance\"] for obj in results20[\"data\"][\"Get\"][\"DocumentChunk\"]]\nplt.figure(figsize=(6,4))\nplt.hist(distances, bins=10, color='orange', edgecolor='black')\nplt.xlabel('Cosine Distance (lower = closer)')\nplt.ylabel('Frequency')\nplt.title('Distribution of distances for top 20 results')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f0a87bb3-0412-405b-bd21-edcc889c35d7",
      "cell_type": "markdown",
      "source": "This might show if there’s a cluster of very close results versus a tail of less relevant ones. Interpreting the visuals: A tight cluster of low-distance (high similarity) results means the query vector found a very specific region in the vector space – often good, as it means the relevant info is clearly differentiated. If the similarities are all somewhat low (say around 0.5–0.6 only), it could indicate the query doesn’t match strongly with any single chunk (maybe the info is scattered or the embedding isn’t capturing it well). In such cases, you might consider whether your chunking could be improved (maybe the relevant info got split) or if the query should be reformulated. Also, comparing the vector vs hybrid approach via visualization can be interesting. For instance, you could plot the hybrid scores alongside vector similarities for the same query. However, since hybrid scores aren’t cosine values, a direct comparison is tricky – but you can at least see their rank order differences.",
      "metadata": {}
    },
    {
      "id": "4d47d258-7c5d-4d01-9765-e93903786f38",
      "cell_type": "markdown",
      "source": "Mini-exercise: After making a change (like a new chunk strategy or a different query), generate a new similarity plot. For example, if you try a query that the system isn’t very confident on, does the chart show a flatter similarity line (indicating uncertainty)? Or if you increase overlap significantly, do you see multiple top results with almost equal similarity (because overlapping chunks contain similar content, thus both rank high)? This visual analysis can guide you in fine-tuning the system further.",
      "metadata": {}
    },
    {
      "id": "db408f2d-b6d8-4d1d-9235-36ca1f1311ea",
      "cell_type": "markdown",
      "source": "9. Conclusion and Next Steps\nIn this workshop, we built a complete Retrieval-Augmented Generation pipeline:\nWeaviate setup and schema – storing chunked data with custom embeddings.\nPDF parsing and chunking – turning a large document into manageable pieces using various strategies.\nEmbedding generation – using a model to vectorize chunks and indexing them for similarity search.\nRetrieval methods – exploring semantic vector search, keyword-based search, and hybrid combinations, plus filtering by metadata.\nEvaluation – using cosine similarity and visual tools to measure retrieval relevance.\nLLM integration – feeding retrieved context to an LLM (GPT-4o-mini) to answer a real technical question.\nThroughout, we suggested experiments to solidify your understanding of how each component affects the outcome.",
      "metadata": {}
    },
    {
      "id": "c6741daf-8c6a-4729-ad20-40dc60cfa40a",
      "cell_type": "markdown",
      "source": "By completing the steps and exercises, you’ve gained experience with chunking best practices, optimizing vector search, and building a QA system that can handle domain-specific queries by augmenting an LLM with external knowledge. This approach can be applied to many scenarios – from technical manuals and product documentation to legal texts or research papers – anywhere you need precise answers from a large document. We encourage you to continue iterating on this system:\nTry using Weaviate’s Generative Module which can do retrieval and generation in one step (offloading some prompt management to Weaviate).\nExplore reranking techniques if you have many relevant chunks (using a second stage model to refine which chunks are truly most relevant to the query).\nIf you have multiple documents, expand the schema (e.g., add a document_title property) and explore multi-document RAG.\nFinally, consider evaluation on a set of Q&A pairs if you can gather some ground-truth answers – this will allow more quantitative measurement of how well your RAG system is performing and where to improve.\nHappy coding, and happy querying with your new RAG system!",
      "metadata": {}
    }
  ]
}